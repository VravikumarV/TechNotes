
Why Kafka ?

Kafka features, FAQs:

1) __consumer_offsets:  is used to store information about committed offsets for each topic:partition per group of consumers (groupID). It is compacted topic, so data will be periodically compressed and only latest offsets information available.
2) _schema - is not a default kafka topic (at least at kafka 8,9). It is added by Confluet.
3) How does the Kafka distribute a topic and its partitions among the brokers?
	When you create a new topic, Kafka places the partitions and replicas in a way that the brokers with least number of existing partitions are used first, and replicas for same partition are on different brokers.
4) Does the Kafka redistribute the topic when a new Kafka Broker joins a cluster?
	When you add a new broker, it is used for new partitions (since it has lowest number of existing partitions), but there is no automatic balancing of existing partitions to the new broker. You can use the replica-reassignment tool to move partitions and replicas to the new broker.
5) Can the topic partition be increased after the topic was created?
	Yes, you can add partitions to an existing topic
6) Do the producers need to know which broker the partition exists and push message to them? What happens when that broker goes down while the producers are pushing messages? Basically, how the producers see the partitions in a huge Kafka cluster?
	When a producer firsts connect to the cluster (via a broker in broker.list parameter), it issues a Metadata request, with the topics you are publishing to, and the broker replies with which broker has which partition. If a broker goes down, the producer detects the failed connection (via tcp timeout usually), and re-issues a metadata request to find the new leader.
7) Kafka does not currently support reducing the number of partitions for a topic.

Examples:
************

Streams:
===========

https://www.programcreek.com/java-api-examples/index.php?api=org.apache.kafka.streams.kstream.KStreamBuilder

https://docs.confluent.io/current/streams/developer-guide/dsl-api.html

https://github.com/spring-cloud/spring-cloud-stream-binder-kafka/issues/265

https://technology.amis.nl/2017/02/11/getting-started-with-kafka-streams-building-a-streaming-analytics-java-application-against-a-kafka-topic/

https://github.com/amient/hello-kafka-streams/blob/master/src/main/java/io/amient/examples/wikipedia/JsonPOJOSerde.java

Connect
===========

https://github.com/amient/hello-kafka-streams/blob/master/src/main/java/io/amient/kafka/connect/embedded/ConnectEmbedded.java

Schema Registry & Avro
========================

Applications & Use Case:
=========================

https://dzone.com/articles/applying-kafka-streams-to-the-purchase-transaction?fromrel=true

https://stackoverflow.com/questions/48247478/real-world-use-cases-where-apache-kafka-is-used



Must Read Links:
******************

https://kafka.apache.org/21/documentation/streams/developer-guide/running-app

https://kafka.apache.org/21/documentation/streams/tutorial

Kafka:

https://stackoverflow.com/questions/36114221/is-it-safe-to-add-partition-or-broker-online-for-kafka?rq=1
https://stackoverflow.com/questions/44014975/kafka-consumer-api-vs-streams-api?rq=1

Streams:
	https://kafka.apache.org/documentation/streams/
	https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/
	

Topics:
*********

Rebalance:


Distributed Mode:


/tmp/kafka-logs:

Configurations:
********************

num.standby.replicas:    replicated local state stores related	[https://kafka.apache.org/21/documentation/streams/developer-guide/running-app]


Kafka Components
********************
Broker
Partitions
Offset
Cluster
leader Node
Replicas



Kafka vs Kafka Streams

Kafka Streams & Tasks

Kafka Connect.

Each task will be assigned a list of partitions from the input streams (i.e., Topics)


Topic - Partitions - Stream - Message - Task - Broker - cluster - Leader

Thread Model

multiple Intsances

Kafka Streams API
**********************







Kafka Connect API
**********************





Connector framework provides an easy to use REST API for retrieval of information about connectors, updating configuration, and managing connectors themselves. 

 Kafka Connect Elasticsearch	[ uses jest ]
 
		 Kafka Connect Elasticsearch which allows sending data from Kafka to Elasticsearch It uses Jest, which is a HTTP based Elasticsearch client library
 
 
 
Resources:
===============

http://kafka.apache.org/documentation.html#theconsumer

https://kafka.apache.org/11/documentation/streams/architecture	[V.Good]


https://sematext.com/blog/kafka-connect-elasticsearch-how-to/	[V.Good]


https://stackoverflow.com/questions/38024514/understanding-kafka-topics-and-partitions		[V.Good]

http://codeflex.co/what-is-apache-kafka/


Interview Questions:
=======================


